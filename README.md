 # 需求1 
* 1、可以根据使用者指定的某些条件，筛选出指定的一些用户（有特定年龄、职业、城市）；<br>
* 2、对这些用户在指定日期范围内发起的session，进行聚合统计，比如，统计出访问时长在0~3s的session占总session数量的比例；<br>
* 3、按时间比例，比如一天有24个小时，其中12:00-13:00的session数量占当天总session数量的50%，当天总session数量是10000个，那么当天总共要抽取1000个session，ok，12:00-13:00的用户，就得抽取1000*50%=500。而且这500个需要随机抽取。<br>
* 4、获取点击量、下单量和支付量都排名10的商品种类<br>
* 5、获取top10的商品种类的点击数量排名前10的session<br>

# 需求分析

* 1、按条件筛选session<br>
* 2、统计出符合条件的session中，访问时长在1s-3s、4s-6s、7s-9s、10s-30s、30s-60s、1m-3m、3m-10m、10m-30m、30m以上各个范围内的session占比；访问步长在1-3、4-6、7-9、10-30、30-60、60以上各个范围内的session占比<br>
* 3、在符合条件的session中，按照时间比例随机抽取1000个session<br>
* 4、在符合条件的session中，获取点击、下单和支付数量排名前10的品类<br>
* 5、对于排名前10的品类，分别获取其点击次数排名前10的session<br>
 
## 1、按条件筛选session
 
搜索过某些关键词的用户、访问时间在某个时间段内的用户、年龄在某个范围内的用户、职业在某个范围内的用户、所在某个城市的用户，发起的session。找到对应的这些用户的session，也就是我们所说的第一步，按条件筛选session。

  这个功能，就最大的作用就是灵活。也就是说，可以让使用者，对感兴趣的和关系的用户群体，进行后续各种复杂业务逻辑的统计和分析，那么拿到的结果数据，就是只是针对特殊用户群体的分析结果；而不是对所有用户进行分析的泛泛的分析结果。比如说，现在某个企业高层，就是想看到用户群体中，28~35岁的，老师职业的群体，对应的一些统计和分析的结果数据，从而辅助高管进行公司战略上的决策制定。

## 2、统计出符合条件的session中，访问时长在1s-3s、4s-6s、7s-9s、10s-30s、30s-60s、1m-3m、3m-10m、10m-30m、30m以上各个范围内的session占比；访问步长在1-3、4-6、7-9、10-30、30-60、60以上各个范围内的session占比

  session访问时长，也就是说一个session对应的开始的action，到结束的action，之间的时间范围；还有，就是访问步长，指的是，一个session执行期间内，依次点击过多少个页面，比如说，一次session，维持了1分钟，那么访问时长就是1m，然后在这1分钟内，点击了10个页面，那么session的访问步长，就是10.

比如说，符合第一步筛选出来的session的数量大概是有1000万个。那么里面，我们要计算出，访问时长在1s-3s内的session的数量，并除以符合条件的总session数量（比如1000万），比如是100万/1000万，那么1s~3s内的session占比就是10%。依次类推，这里说的统计，就是这个意思。

这个功能的作用，其实就是，可以让人从全局的角度看到，符合某些条件的用户群体，使用我们的产品的一些习惯。比如大多数人，到底是会在产品中停留多长时间，大多数人，会在一次使用产品的过程中，访问多少个页面。那么对于使用者来说，有一个全局和清晰的认识。

## 3、在符合条件的session中，按照时间比例随机抽取1000个session

  这个按照时间比例是什么意思呢？随机抽取本身是很简单的，但是按照时间比例，就很复杂了。比如说，这一天总共有1000万的session。那么我现在总共要从这1000万session中，随机抽取出来1000个session。但是这个随机不是那么简单的。需要做到如下几点要求：首先，如果这一天的12:00~13:00的session数量是100万，那么这个小时的session占比就是1/10，那么这个小时中的100万的session，我们就要抽取1/10 * 1000 = 100个。然后再从这个小时的100万session中，随机抽取出100个session。以此类推，其他小时的抽取也是这样做。

  这个功能的作用，是说，可以让使用者，能够对于符合条件的session，按照时间比例均匀的随机采样出1000个session，然后观察每个session具体的点击流/行为，比如先进入了首页、然后点击了食品品类、然后点击了雨润火腿肠商品、然后搜索了火腿肠罐头的关键词、接着对王中王火腿肠下了订单、最后对订单做了支付。

  之所以要做到按时间比例随机采用抽取，就是要做到，观察样本的公平性。

## 4、在符合条件的session中，获取点击、下单和支付数量排名前10的品类

  什么意思呢，对于这些session，每个session可能都会对一些品类的商品进行点击、下单和支付等等行为。那么现在就需要获取这些session点击、下单和支付数量排名前10的最热门的品类。也就是说，要计算出所有这些session对各个品类的点击、下单和支付的次数，然后按照这三个属性进行排序，获取前10个品类。
  这个功能，很重要，就可以让我们明白，就是符合条件的用户，他最感兴趣的商品是什么种类。这个可以让公司里的人，清晰地了解到不同层次、不同类型的用户的心理和喜好。

## 5、对于排名前10的品类，分别获取其点击次数排名前10的session

  这个就是说，对于top10的品类，每一个都要获取对它点击次数排名前10的session。

  这个功能，可以让我们看到，对某个用户群体最感兴趣的品类，各个品类最感兴趣最典型的用户的session的行为。


 # 需求2
 * 1、接收J2EE系统传入进来的taskid，从mysql查询任务的参数，日期范围、页面流id<br>
 * 2、针对指定范围日期内的用户访问行为数据，去判断和计算，页面流id中，每两个页面组成的页面切片，它的访问量是多少<br>
 * 3、根据指定页面流中各个页面切片的访问量，计算出来各个页面切片的转化率<br>
 * 4、计算出来的转化率，写入mysql数据库中<br>

# 需求分析
* 用户指定的页面流id：3,5,7,9,10,21<br>
* 页面3->页面5的转换率是多少；<br>
* 页面5->页面7的转化率是多少；<br>
* 页面7->页面9的转化率是多少；<br>
* 页面3->页面5的访问量是多少；<br>
* 页面5到页面7的访问量是多少；<br>
* 两两相除，就可以计算出来<br>

 # 实现
* 1、获取任务的日期范围参数<br>
* 2、查询指定日期范围内的用户访问行为数据<br>
* 3、获取用户访问行为中，每个session，计算出各个在指定页面流中的页面切片的访问量；实现，页面单跳切片生成以及页面流匹配的算法；session，3->8->7，3->5->7，是不匹配的；<br>
* 4、计算出符合页面流的各个切片的pv（访问量）<br>
* 5、针对用户指定的页面流，去计算各个页面单跳切片的转化率<br>
* 6、将计算结果持久化到数据库中（备注：数据表，其实是比较简单的, taskid：唯一标识一个任务,convert_rate：页面流中，各个页面切片的转化率，以特定的格式拼接起来，作为这个字段的值 3,5=10%;5,7=20%）<br>


 # 需求3
 ## 根据用户指定的日期范围，统计各个区域下的最热门的top3商品
* 1、区域信息在哪里，各个城市的信息，城市是不怎么变化的，没有必要存储在hive里？MySQL，Hive和MySQL异构数据源使用，技术点<br>
* 2、hive用户行为数据，和mysql城市信息，join，关联之后是RDD？RDD转换DataFrame，注册临时表，技术点<br>
* 3、各个区域下各个商品的点击量，保留每个区域的城市列表数据？自定义UDAF函数，group_concat_distinct()<br>
* 4、product_id，join hive表中的商品信息，商品信息在哪里？Hive。商品的经营类型是什么？自定义UDF函数，get_json_object()，if()<br>
* 5、获取每个区域的点击量top3商品？开窗函数；给每个区域打上级别的标识，西北大区，经济落后，区域上的划分，C类区域；北京、上海，发达，标记A类<br>
* 6、Spark SQL的数据倾斜解决方案？双重group by、随机key以及扩容表（自定义UDF函数，random_key()）、内置reduce join转换为map join、shuffle并行度<br>

## 技术方案设计：
* 1、查询task，获取日期范围，通过Spark SQL，查询user_visit_action表中的指定日期范围内的数据，过滤出，商品点击行为，click_product_id is not null；click_product_id != 'NULL'；click_product_id != 'null'；city_id，click_product_id<br>
* 2、使用Spark SQL从MySQL中查询出来城市信息（city_id、city_name、area），用户访问行为数据要跟城市信息进行join，city_id、city_name、area、product_id，RDD，转换成DataFrame，注册成一个临时表<br>
* 3、Spark SQL内置函数（case when），对area打标记（华东大区，A级，华中大区，B级，东北大区，C级，西北大区，D级），area_level<br>
* 4、计算出来每个区域下每个商品的点击次数，group by area, product_id；保留每个区域的城市名称列表；自定义UDAF，group_concat_distinct()函数，聚合出来一个city_names字段，area、product_id、city_names、click_count<br>
* 5、join商品明细表，hive（product_id、product_name、extend_info），extend_info是json类型，自定义UDF，get_json_object()函数，取出其中的product_status字段，if()函数（Spark SQL内置函数），判断，0 自营，1 第三方；（area、product_id、city_names、click_count、product_name、product_status）<br>
* 6、开窗函数，根据area来聚合，获取每个area下，click_count排名前3的product信息；area、area_level、product_id、city_names、click_count、product_name、product_status
* 7、结果写入MySQL表中
* 8、Spark SQL的数据倾斜解决方案？双重group by、随机key以及扩容表（自定义UDF函数，random_key()）、Spark SQL内置的reduce join转换为map join、提高shuffle并行度<br>
* 9、本地测试和生产环境的测试<br>

 # 需求4
* 1、实现实时的动态黑名单机制：将每天对某个广告点击超过100次的用户拉黑<br>
* 2、基于黑名单的非法广告点击流量过滤机制：<br>
* 3、每天各省各城市各广告的点击流量实时统计：<br>
* 4、统计每天各省top3热门广告<br>
* 5、统计各广告最近1小时内的点击量趋势：各广告最近1小时内各分钟的点击量<br>
* 6、使用`高性能`方式将实时统计结果写入MySQL<br>
* 7、实现实时计算程序的HA高可用性（Spark Streaming HA方案）<br>
* 8、实现实时计算程序的性能调优（Spark Streaming Performence Tuning方案）<br>

# 技术方案设计
* 1、实时计算各batch中的每天各用户对各广告的点击次数<br>
* 2、使用`高性能`方式将每天各用户对各广告的点击次数写入MySQL中（更新）<br>
* 3、使用filter过滤出每天对某个广告点击超过100次的黑名单用户，并写入MySQL中<br>
* 4、使用transform操作，对每个batch RDD进行处理，都动态加载MySQL中的黑名单生成RDD，然后进行join后，过滤掉batch RDD中的黑名单用户的广告点击行为<br>
* 5、使用updateStateByKey操作，实时计算每天各省各城市各广告的点击量，并时候更新到MySQL<br>
* 6、使用transform结合Spark SQL，统计每天各省份top3热门广告：首先以每天各省各城市各广告的点击量数据作为基础，首先统计出每天各省份各广告的点击量；然后启动一个异步子线程，使用Spark SQL动态将数据RDD转换为DataFrame后，注册为临时表；最后使用Spark SQL开窗函数，统计出各省份top3热门的广告，并更新到MySQL中<br>
* 7、使用window操作，对最近1小时滑动窗口内的数据，计算出各广告各分钟的点击量，并更新到MySQL中<br>
* 8、实现实时计算程序的HA高可用性<br>
* 9、对实时计算程序进行性能调优<br>

